% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/std_coeffs.R
\name{R2}
\alias{R2}
\title{R-squared/Pseudo R-squared}
\usage{
R2(m, data = NULL, adj = FALSE, pred = FALSE, re.form = NA, ...)
}
\arguments{
\item{m}{A fitted model object of class \code{lm}, \code{glm}, or
\code{merMod}, or a list or nested list of such objects.}

\item{data}{An optional dataset used to first re-fit the model(s).}

\item{adj}{Logical, should adjusted R squared also be calculated?}

\item{pred}{Logical, should predictive R squared also be calculated?}

\item{re.form}{For mixed models of class \code{merMod}, the formula for
random effects to condition on when generating model fitted values used in
the calculation of R-squared. Defaults to \code{NA}, meaning no random
effects are included. See \code{?lme4::predict.merMod} for further
information.}

\item{...}{Not used.}
}
\value{
A numeric vector of the R-squared value(s), or an array, list or
  nested list of such vectors.
}
\description{
Calculate R-squared or pseudo R-squared for a fitted model as
  the squared multiple correlation between the observed and fitted values for
  the response variable. 'Adjusted' and 'predicted' R-squared values can
  optionally also be calculated (see Details).
}
\details{
Various approaches to the calculation of a goodness-of-fit measure
  for GLM's analogous to R-squared in the ordinary linear model have
  been proposed. Generally termed 'pseudo R-squared' measures, they include
  variance-based, likelihood-based, and distribution-specific approaches.
  Here however, a straightforward OLS definition is used, which can be
  applied to any type of model for which predicted values of the response
  variable are generated: R-squared is calculated as the squared (weighted)
  correlation between the observed and predicted values of the response (in
  the original scale). This is simply the squared version of the correlation
  measure advocated by Zheng & Agresti (2000), itself an intuitive measure of
  goodness-of-fit describing the predictive power of a model. As the measure
  does not depend on any specific error distibution or model estimating
  procedure, it is also generally comparable across many different types of
  model (Kvalseth 1985). In the case of the ordinary linear model, the
  measure equals the more 'traditional' R-squared based on sums of squares.

  If argument \code{adj} is set to \code{TRUE}, the adjusted R-squared value
  is also returned, which provides an estimate of the population (as opposed
  to sample) R-squared via an analytical formula which adjusts R-squared for
  the degrees of freedom of the model. Here this is calculated here via the
  'Pratt' rather than standard ('Ezekiel/Wherry') formula, shown in a
  simulation study to be the most effective of a range of existing formulas
  at estimating the population R-squared, across a broad range of model
  specification scenarios (Yin & Fan 2001).

  If \code{pred = TRUE}, then a 'predictive' R-squared is also returned,
  which is calculated via the same formula as for R-squared but using
  'cross-validated' rather than standard model predictions. These are
  obtained by dividing model residuals by the complement of the observation
  leverage values (diagonals of the hat matrix), then subtracting these
  inflated residuals from the response variable. This is essentially a short
  cut to obtaining 'out-of-sample' predictions, normally arising via a
  leave-one-out cross validation procedure (in a GLM however they will not be
  exactly equal to such predictions). The resulting R-squared is an estimate
  of the R-squared that would occur were the model to be fitted to new data,
  and will typically be lower than the original R-squared - highlighting the
  degree of overfitting in the original model.

  For mixed models, the function will, by default, calculate all R-squared
  values using fitted values for the fixed effects only, meaning that random
  effects are averaged over. This is the measure likely of primary interest
  to most researchers, and is equivalent to the 'marginal' R-squared
  statistic of Nakagawa \emph{et al.} (2017). In order to incorporate some or
  all of the random effects however, simply set the appropriate formula using
  the argument \code{re.form} (see \code{\link[lme4]{predict.merMod}}).
  Setting this to \code{NULL} includes all random effects, making the measure
  equivalent to the 'conditional' R-squared of Nakagawa \emph{et al.} (2017).

  R-squared values produced by this function will always be bounded between
  zero (no fit) and one (perfect fit), meaning that any negative values
  arising from calculations are rounded up to zero. Negative values for
  R-squared typically mean that the fit is 'worse' than the null expectation
  of no relationship between the variables, which is difficult to interpret
  in practice and in any case usually only occurs in rare situations, such as
  where the intercept is suppressed. Hence, for simplicity and ease of
  interpretation, values <= 0 here are presented as a complete lack of model
  fit.

  PLEASE NOTE: caution must be exercised in interpreting the values of any
  (pseudo) R-squared measure calculated for a GLM or mixed model (including
  those produced by this function), as such measures do not hold all the
  properties of R-squared in the ordinary linear model and as such may not
  always behave as expected. They are, at best, approximations. However, the
  value of a standardised measure of fit for model assessment and comparison
  may outweigh such reservations.
}
\examples{

}
\references{
Allen, D. M. (1974). The Relationship Between Variable Selection
  and Data Agumentation and a Method for Prediction. \emph{Technometrics},
  \strong{16}(1), 125-127.
  \url{https://doi.org/10.1080/00401706.1974.10489157}

  Kvalseth, T. O. (1985) Cautionary Note about R2. \emph{The American
  Statistician}, \strong{39}(4), 279-285.
  \url{https://doi.org/10.2307/2683704}

  Nakagawa, S., Johnson, P.C.D. and Schielzeth, H. (2017) The coefficient of
  determination R2 and intra-class correlation coefficient from generalized
  linear mixed-effects models revisited and expanded. \emph{Journal of the
  Royal Society Interface} \strong{14}(134).
  \url{https://doi.org/10.1098/RSIF.2017.0213}

  Yin, P. and Fan, X. (2001) Estimating R2 Shrinkage in Multiple Regression:
  A Comparison of Different Analytical Methods. \emph{The Journal of
  Experimental Education} \strong{69}(2), 203-224.
  \url{https://doi.org/10.1080/00220970109600656}

  Zheng, B. and Agresti, A. (2000) Summarizing the predictive power of a
  generalized linear model. \emph{Statistics in Medicine} \strong{19}(13),
  1771-1781.
  \url{https://doi.org/10.1002/1097-0258(20000715)19:13<1771::AID-SIM485>3.0
  .CO;2-P}
}
