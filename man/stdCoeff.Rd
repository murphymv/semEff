% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/std_coeffs.R
\name{stdCoeff}
\alias{stdCoeff}
\title{Standardised Coefficients}
\usage{
stdCoeff(m, weights = NULL, data = NULL, term.names = NULL,
  cen.x = TRUE, cen.y = TRUE, std.x = TRUE, std.y = TRUE,
  unique.x = TRUE, r.squared = FALSE, ...)
}
\arguments{
\item{m}{A fitted model object of class \code{lm}, \code{glm}, or
\code{merMod}, or a list or nested list of such objects.}

\item{weights}{An optional numeric vector of weights to use for model
averaging, or a named list of such vectors. The former should be supplied
when \code{m} is a list, and the latter when it is a nested list (with
matching list names). If set to \code{"equal"}, a simple average is
calculated instead.}

\item{data}{An optional dataset used to first re-fit the model(s).}

\item{term.names}{An optional vector of term names used to extract and/or
sort coefficients from the output.}

\item{cen.x}{Logical, whether the intercept and coefficients should be
calculated as if from mean-centred predictor variables.}

\item{cen.y}{Logical, whether the intercept should be calculated as if the
response variable was mean-centred.}

\item{std.x}{Logical, whether coefficients should be scaled by the standard
deviations of the predictors.}

\item{std.y}{Logical, whether coefficients should be scaled by the standard
deviation of the response.}

\item{unique.x}{Logical, whether coefficients should be adjusted for
multicollinearity among predictors.}

\item{r.squared}{Logical, whether R-squared values should also be returned.}

\item{...}{Arguments to function \code{R2}.}
}
\value{
A numeric vector of the standardised coefficients, or a list or
  nested list of such vectors.
}
\description{
Calculate fully standardised model coefficients in standard
  deviation units, adjusted for multicollinearity among predictors.
}
\details{
This function will calculate fully standardised coefficients in
  standard deviation units for linear, generalised linear, and mixed models
  (of class \code{merMod}). It achieves this via adjusting the 'raw' model
  coefficients, so no standardisation of input variabes is required
  beforehand. Users can simply specify the model with all variables in their
  original units and the function will do the rest. However, the user is free
  to scale and/or centre any input variables should they choose, which will
  not affect the outcome of standardisation (provided any scaling is by
  standard deviations). This may be desirable in some cases, such as to
  increase numerical stability during model fitting when variables are on
  widely different scales.

  If arguments \code{cen.x} or \code{cen.y} are \code{TRUE}, model estimates
  will be calculated as if all predictors (x) and/or the response variable
  (y) were mean-centred prior to model-fitting. Thus, for an ordinary linear
  model where centring of x and y is specified, the intercept will equal zero
  (the (weighted) mean of y). In addition, if \code{cen.x = TRUE} and there
  are interacting terms in the model, all coefficients for lower order terms
  of the interation are adjusted using an expression which ensures that each
  main effect or lower order term is estimated at the mean values of the
  terms they interact with (zero in a 'centred' model) - typically improving
  the interpretation of coefficients. The expression used comprises a
  weighted sum of all the coefficients that contain the lower order term,
  with the weight for the term itself being zero and those for 'containing'
  terms being the product of the means of the other variables involved in
  that term (i.e. those not in the lower order term itself). For example, for
  a three-way interaction (x1 * x2 * x3), the expression for main effect
  \eqn{\beta1} would be:

  \deqn{\beta_{1} + \beta_{12} \bar{x}_{2} + \beta_{13} \bar{x}_{3} +
  \beta_{123} \bar{x}_{2} \bar{x}_{3}}{\beta1 + (\beta12 * Mx2) + (\beta13 *
  Mx3) + (\beta123 * Mx2 * Mx3)} (adapted from here:
  \url{http://bit.ly/2FOJPk8})

  In addition, if \code{std.x = TRUE} or \code{unique.x = TRUE} (see below),
  product terms for interactive effects will be re-calculated using
  mean-centred main effects, to ensure that standard deviations and variance
  inflation factors for predictors are calculated correctly (the model is
  re-fit for this latter purpose, to recalculate the variance-covariance
  matrix).

  If \code{std.x = TRUE}, coefficients are standardised by multiplying by the
  standard deviations of predictor variables (or terms), while if \code{std.y
  = TRUE} they are divided by the standard deviation of the response. If the
  model is a GLM, this latter is calculated from the link-transformed
  response (or an estimate of same) generated using the function \code{getY}.
  If both arguments are true, the coefficients are regarded as 'fully'
  standardised in the traditional sense, often referred to as 'betas'.

  If \code{unique.x = TRUE}, coefficients are adjusted for multicollinearity
  among predictors by dividing by the square root of the variance inflation
  factors (Dudgeon 2016, Thompson \emph{et al.} 2017). If they are also
  standardised by the standard deviations of x and y, this converts them to
  semipartial correlations, i.e. the correlation between the unique
  components of predictors (residualised on other predictors) and the
  response variable. This measure of effect size is arguably much more
  interpretable and useful than the traditional standardised coefficient, as
  it is always estimated independent of the effects of multicollinearity, and
  so values can more readily be compared both within and across models. The
  effect size ranges from zero (no effect) to +/-1 (perfect relationship),
  rather than from zero to +/- infinity (as in the case of betas) - putting
  it on the same scale as the bivariate correlation between predictor and
  response. In the case of GLM's however, the measure is analagous but not
  exactly equal to the semipartial correlation, so its value may not be
  always be bound between +/-1 (such cases are likely rare). Crucially, for
  ordinary linear models, the square of the semipartial correlation equals
  the increase in R-squared when that variable is added last in the model -
  directly linking the measure to model fit and 'variance explained'. See
  \url{http://bit.ly/2GmyrMA} for additional arguments for the use of
  semipartial correlations.

  If \code{r.squared = TRUE}, R-squared values are also returned via the
  \code{R2} function.

  Finally, if \code{weights} are specified, the function calculates a
  weighted average of the standardised coefficients across models (Burnham &
  Anderson 2002).
}
\references{
Burnham, K. P., & Anderson, D. R. (2002). \emph{Model Selection
 and Multimodel Inference: A Practical Information-Theoretic Approach} (2nd
 ed.). New York: Springer-Verlag. Retrieved from
 \url{https://www.springer.com/gb/book/9780387953649}

 Dudgeon, P. (2016). A Comparative Investigation of Confidence Intervals for
 Independent Variables in Linear Regression. \emph{Multivariate Behavioral
 Research}, \strong{51}(2-3), 139-153.
 \url{https://doi.org/10.1080/00273171.2015.1121372}

 Thompson, C. G., Kim, R. S., Aloe, A. M., & Becker, B. J. (2017). Extracting
 the Variance Inflation Factor and Other Multicollinearity Diagnostics from
 Typical Regression Results. \emph{Basic and Applied Social Psychology},
 \strong{39}(2), 81-90. \url{https://doi.org/10.1080/01973533.2016.1277529}
}
\seealso{
\code{\link[stats]{coef}}, \code{\link[semEff]{VIF}},
  \code{\link[semEff]{getY}}, \code{\link[semEff]{R2}},
  \code{\link[semEff]{avgEst}}
}
